<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="eP-ALM">
  <meta name="keywords" content="eP-ALM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>eP-ALM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body publication-header">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1">eP-ALM: Efficient Perceptual Augmentation of Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=lhp9mRgAAAAJ&view_op=list_works&sortby=pubdate">Mustafa Shukor<sup>a</sup></a>, 
            </span>
            <span class="author-block">
              <a href="https://cdancette.fr/">Corentin Dancette<sup>a</sup></a>, 
            </span>
            <span class="author-block">
              <a href="https://cord.isir.upmc.fr/">Matthieu Cord<sup>a,b</sup></a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">a) Sorbonne University  &nbsp; b) Valeo.ai &nbsp; </span>
          </div>

          <!-- <div class="is-size-10 publication-authors">
            (*Equal contribution)
          </div> -->
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://aps.arxiv.org/abs/2303.11403"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/mshukor/eP-ALM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src = "static/images/colab-logo.svg" alt="colab-logo"/>
                  </span>
                  <span>Colab</span>
                  </a>
              </span> -->


              <span class="link-block">
                <a href="https://huggingface.co/spaces/mshukor/eP-ALM?logs=build"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src = "static/images/hf-logo.svg" alt="hf-logo"/>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>


              <!-- Code Link. -->
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>BibTex</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<div align="center" style="margin-top:0px; margin-bottom:0px;">
  <div class="teaser-padding">
    <div class="hero-body">
      <div class="container">
        <!-- <div id="results-carousel" class="carousel results-carousel"> -->

        <figure class="teaser">
          <img class="teaser-image" style='height: auto; width: 50%; object-fit: contain' src="static/images/teaser.jpg"/>
          <!-- <figcaption class="teaser-overlay">
            <div class="teaser-meta">
              <span class="teaser-title">This is.</span>
              <p class="teaser-description">Lilith is .</p>
            </div>
          </figcaption> -->
        </figure>

          <!-- <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
              <source type="video/mp4" src="static/videos/teaser_video_1_compressed.mp4" />
            </video>
          </div> -->
        
        <!-- </div> -->
      </div>
      <h3 class="subtitle has-text-centered">
        <div class="column is-four-fifths">  
        <font size="3">
          <div align="center" style="margin-top:0px; margin-bottom:0px;">
          <span class="dnerf"></span>
          Illustration of eP-ALM to adapt unimodal models for multimodal tasks. 
          The Language Model (Decoder) is augmented with perceptual context to steer its text generation. 
          To condition the decoder on a given modality, the [CLS] tokens are extracted from several layers of a modality-specific encoder and 
          then linearly projected before concatenation at different levels of the language decoder. 
          Only unimodal models are used, and all pretrained modules are kept frozen.
        </div>
        </font>
      </div>
      </h3>
    </div>
  </div>
</div>  

<!-- <div align="center" style="margin-top:0px; margin-bottom:0px;">
  <div class="carousel-teaser-padding">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
                <source type="video/mp4" src="static/videos/teaser_video_0_compressed.mp4" /> 
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
              <source type="video/mp4" src="static/videos/teaser_video_4_compressed.mp4" /> 
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
              <source type="video/mp4" src="static/videos/teaser_video_2_compressed.mp4" />
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
              <source type="video/mp4" src="static/videos/teaser_video_3_compressed.mp4" />
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline muted loop>
              <source type="video/mp4" src="static/videos/teaser_video_1_compressed.mp4" />
            </video>
          </div>
        
        </div>
      </div>
      <h3 class="subtitle has-text-centered">
          <font size="4">
          <span class="dnerf"></span>
          Without explict supervision, Diffusion Features can find correspondences on real images across instances, categories, and even domains.
        </font>
      </h3>
    </div>
  </div>
</div>  -->


<section class="section grey">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. 
            On the vision side, transformer models (\emph{i.e.}, ViT) are following the same trend, achieving the best performance on challenging benchmarks. 
            With the abundance of such unimodal models, a natural question arises; <strong><i> do we also need to follow this trend to tackle multimodal tasks? </i></strong> 
            In this work, we propose to rather <i>direct effort to efficient adaptations of existing models</i>, and propose to augment Language Models with perception. 
            Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. 
            In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. 
            In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with little to no effort on direct finetuning. 
            We investigate the minimal computational effort needed to adapt unimodal models for multimodal tasks and propose a new challenging setup, alongside different approaches, 
            that efficiently adapts unimodal pretrained models. We show that by freezing more than <strong> 99% </strong> of total parameters, 
            training only one linear projection layer, and prepending only one trainable token, 
            our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and Captioning across Image, Video, and Audio modalities, following the proposed setup, 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<br>
<br>

<section class="section">
  <div class="columns is-centered">
    <div class="column is-four-fifths">
      <h3 class="title is-3">Findings and Contributions</h3>
      <div class="content has-text-justified">
        <p>
          <ul>
          <li>Training a single linear layer directly on downstream multimodal datasets, and following the same setup, 
            outperforms other work on Image/Video/Audio-Language tasks. With a few additional trainable parameters and a 
            single learned prepended token, we can significantly improve the performance, while respecting a budget of 1 %  of trainable parameters, 
            and keeping almost the same inference cost.</li>

          <li>With only a single learned prepended token, performances increase with almost the same inference cost.</li>
      
          <li>Our approach enjoys better generalization (OOD, Zero-Shot) and is data-efficient (training on 1 % of the data achieves 80 % of performances) 
            with better few-shot results than other approaches.</li>
      
          <li>While reaching good performance with small to mid-scale language models (i.e, 350M-2.7B) the improvement still 
            increases by jointly scaling both vision and language models. When scaling both models, we can still outperform other approaches 
            with only 0.06 % of trainable parameters.</li>
      
          <li>Existing approaches do not behave well on the proposed challenging setup, without large multi-modal pretraining.</li>

        </ul>
        </p>
      </div>
    </div>
  </div>

</section>

<!-- Method -->
<section class="section grey">
  <div class="columns is-centered has-text-centered">
    <h3 class="title is-3 margin-bottom-8">Model </h3>
  </div>
  
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">

          
          <figure class="teaser">
            <div align="center" style="margin-top:0px; margin-bottom:0px;"></div>
            <img class="teaser-image" style='height: auto; width: 50%; object-fit: contain;' src="static/images/arch.jpg"/>
            <!-- <figcaption class="teaser-overlay">
              <div class="teaser-meta">
                <span class="teaser-title">This is.</span>
                <p class="teaser-description">Lilith is .</p>
              </div>
            </figcaption> -->
          </figure>


          <p> 
            We adopt OPT models, which are autoregressive language decoders consisting of Self-Attention and Feed Forward layers. 
            They are trained with <i>next token prediction</i> objective on 180B tokens mostly in English and gathered from different datasets. Our model is based on the following main points:
            <br>
            <br>
            <strong>Perceptual Encoders:</strong> we favor only unimodal models. For images, we use the vanilla ViT model pretrained for image classification 
            on ImageNet. For Video, we use TimeSformer that consists of a ViT-like model augmented with temporal attention and pretrained on kinetics. 
            For Audio, we adopt AST, a vanilla adaptation of ViT to digest spectrograms, that is pretrained on AudioSet. 
            
            <br>
            <br>
            <strong>Perceptual Prompt Injection:</strong>  LMs are usually controlled via different textual prompts, such as questions and instructions. 
            Here, the LM is controlled by both the text and the perceptual encoders. Specifically, the projected perceptual tokens are prepended to the 
            textual tokens. Naively using all visual tokens, adds significant computation costs during training and inference.
            To mitigate this, we consider only the [CLS] token of the perceptual encoders and prepend it to the text tokens. 
            This increases the total number of tokens by 1 which maintains almost the same inference speed.
            <br>
            <br>
            <strong>Connecting Models with Cross-Modal Hierarchical Linear layers:</strong> when freezing the perceptual encoders and language models, 
            the minimal number of trainable parameters are those that amount to connecting these two models while adjusting the embedding dimensions in case 
            of a mismatch. Therefore, we base our approach on this constraint and train only one linear projection layer to connect both models. 
            To exploit the hierarchical representation encoded in pretrained models, instead of taking only the [CLS] token of the last output layer, 
            we take the [CLS] tokens from several layers of the perceptual model, and we inject these tokens into several layers of the LM (shared connection). 
            The tokens coming from early layers are injected earlier and are then replaced by those coming from deeper layers. 
            We favor only the deeper layers where the representations are more abstract and less modality-specific. 
            Moreover, using the same linear projection at different representation levels might not help 
            to capture the particularity of such a hierarchy, to this end, we also experiment with different linear layers for each 
            level. 
            <br>
            <br>
            <strong>Multimodal Adaptation with Parameter-Efficient Techniques:</strong> we explore several parameter-efficient techniques to ease 
            the adaptation to multimodal tasks. The main technique we use is <i>Prompt Tuning</i>: it consists of prepending trainable tokens or 
            Soft Prompts to the textual tokens input of the LM. This gives useful context to steer the model output. 
            Contrary to hard prompts that are manually engineered, this provides a more flexible and easier approach for task-dependant contextualization. 
            For the sake of efficiency, we prepend only 10 learnable tokens. We also experiment <i>Adapters</i> .
          </p>
        </div>
      </div>
    </div>



    
  <br>

  <!-- <div class="columns is-centered has-text-centered">
    <h3 class="title is-3">Image Editing with DIFT</h3>
  </div>
 <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p> DIFT can easily propagate edits from one image to others across different instances, categories, and domains, without any correspondence supervision. </p>
        </div>
      </div>
    </div>
  <div class="carousel-extra-padding">
    <div class="hero-body carousel-body-vert-padding">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-toby">
            <video poster="" autoplay playsinline controls muted loop>
                <source type="video/mp4" src="static/videos/image_edit_dog_compressed.mp4" /> 
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline controls muted loop>
              <source type="video/mp4" src="static/videos/image_edit_cat_compressed.mp4" /> 
            </video>
          </div>

          <div class="item item-toby">
            <video poster="" autoplay playsinline controls muted loop>
              <source type="video/mp4" src="static/videos/image_edit_bird_compressed.mp4" />
            </video>
          </div>
        
        </div>
      </div>
    </div>
  </div> -->
</section>


<!-- ############ -->
<section class="section hero is-small">
  <div class="columns is-centered has-text-centered">
    <h2 class="title is-3">Experiments</h2>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p> We compare with other end-to-end trained SoTA that takes only
            the audio signal as input and show that our approach is
            very competitive with previous work, showing the potential
            of efficient adaptation of LM for the audio modality..</p>
        </div>
      </div>
  </div>

  <figure class="teaser center">
    <div align="center" style="margin-top:0px; margin-bottom:0px;">
      <img class="teaser-image" style='height: auto; width: 40%; object-fit: contain' src="static/images/audio.png"/>
      <figcaption class="teaser-overlay">
        <div class="teaser-meta">
          <span class="teaser-title">Comparison with other work that train most of the model parameters for Audio Captioning on AudioCaps
            Test set.</span>
          <!-- <p class="teaser-description"> Comparison with other work that train most of the model parameters for Audio Captioning on AudioCaps
            Test set.</p> -->
        </div>
      </figcaption>
    </div>
  </figure>





<!-- ############ -->

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <div class="content has-text-justified">
        <p> To explore the generalization of our approach, we evaluate on Zero-Shot for VideoQA, where the model is trained on a dataset 
          different from the target one. Table below shows that eP-ALM, trained on VQA v2 (standard split), outperforms other approaches 
          trained on significantly more data. 
          Contrary to some of other approaches that cast the task as classification (similarity-based)  or constrained generation through masking, 
          considering only a subset of answers (1k or 2k), our approach is evaluated (with a character-wise comparison with the ground-truth) 
          with unconstrained Open-ended Generation (OE Gen) and can generate answers with arbitrary lengths. </p>
      </div>
    </div>
</div>

<figure class="teaser center">
  <div align="center" style="margin-top:0px; margin-bottom:0px;">
    <img class="teaser-image" style='height: auto; width: 40%; object-fit: contain' src="static/images/zs_vid.png"/>
    <figcaption class="teaser-overlay">
      <div class="teaser-meta">
        <span class="teaser-title">Zero-Shot results on Video QA. OE Gen: unconstrained Open-
          Ended Generation. † evaluated on questions with top 1k answers.</span>
        <!-- <p class="teaser-description"> Zero-Shot results on Video QA. OE Gen: unconstrained Open-
          Ended Generation. † evaluated on questions with top 1k answers.</p> -->
      </div>
    </figcaption>
  </div>
</figure>


<!-- ############ -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      <p> Here we investigate whether our parameter-efficient approach can perform well in OOD scenarios.
         To this end, we follow other approaches  and train our model on the training set of a given benchmark, and evaluate it on the validation set of 
         another benchmark, without multimodal pretraining. We measure the performance gap, i.e. the accuracy difference between a model trained 
         on a different benchmark and the same model trained on the target benchmark.
         Table below shows that eP-ALM, which trains 0.06\% of total parameters, is very competitive in terms of OOD accuracy with other baselines, 
         that train all model parameters and pretrain on large amount of data. 
         This reveals that our parameter-efficient approach generalizes relatively well in OOD scenarios.</p>
    </div>
  </div>
</div>

<figure class="teaser center">
  <div align="center" style="margin-top:0px; margin-bottom:0px;">
  <img class="teaser-image" style='height: auto; width: 30%; object-fit: contain' src="static/images/vqa_gen.png"/>
  <figcaption class="teaser-overlay">
    <div class="teaser-meta">
      <span class="teaser-title">Out-Of-Distribution Generalization on GQA and VQA v2 (stan-
        dard split). The Gap shows the performance degradation when the model is
        trained on a different dataset.</span>
      <!-- <p class="teaser-description"> Out-Of-Distribution Generalization on GQA and VQA v2 (stan-
        dard split). The Gap shows the performance degradation when the model is
        trained on a different dataset.</p> -->
    </div>
  </div>
  </figcaption>
</figure>

<!-- ############ -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      <p> In this section, we investigate how data-efficient our model can be.
         To this end, we train on a very small portion (randomly sampled) from the VQA training set and evaluate on the validation set.  
         Table below, shows the superiority of our approach over other baselines. 
         Interestingly, we can achieve 80\% (41.9 vs 52.77) of the performance when training on 1\% of the data. 
         This validates the approach on low resources scenarios and shows that, in addition to being parameter-efficient, our model is also data-efficient.</p>
    </div>
  </div>
</div>

<figure class="teaser center">
  <div align="center" style="margin-top:0px; margin-bottom:0px;">
  <img class="teaser-image" style='height: auto; width: 30%; object-fit: contain' src="static/images/fewshot.png"/>
  <figcaption class="teaser-overlay">
    <div class="teaser-meta">
      <span class="teaser-title">Few-shot Results on VQA v2 validation set (standard split). ∗:
        longer training.</span>
      <!-- <p class="teaser-description"> Few-shot Results on VQA v2 validation set (standard split). ∗:
        longer training.</p> -->
    </div>
  </figcaption>
</div>
</figure>
<!-- ############ -->

<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      <p> To contextualize the work, we compare eP-ALM to other SoTA that trains large number of parameters and most often with large-scale pretraining. 
        Table below shows a comparison with both zero-shot (ZS) and Finetuning (FT) setups. 
        The performance of eP-ALM is generally higher than ZS scores and still below FT ones. 
        However, the performance gap with FT models, is smaller with the audio and video modalities.</p>
    </div>
  </div>
</div>

<figure class="teaser center">
  <div align="center" style="margin-top:0px; margin-bottom:0px;">
  <img class="teaser-image" style='height: auto; width: 40%; object-fit: contain' src="static/images/sota.png"/>
  <figcaption class="teaser-overlay">
    <div class="teaser-meta">
      <span class="teaser-title">Comparison of eP-ALM with text generation-based SoTA
        that train significant number of parameters, including methods with
        large-scale pretraining. Best and next best scores are bolded and
        underlined respectively. FT: Finetuning. ZS: Zero-shot.</span>
      <!-- <p class="teaser-description"> Comparison of eP-ALM with text generation-based SoTA
        that train significant number of parameters, including methods with
        large-scale pretraining. Best and next best scores are bolded and
        underlined respectively. FT: Finetuning. ZS: Zero-shot.</p> -->
    </div>
  </figcaption>
</div>
</figure>
<!-- ############ -->


</section>





<br>
<br>

<section class="section hero is-small">
  <div class="columns is-centered has-text-centered">
    <h2 class="title is-3">Qualitative Results </h2>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <!-- <p> By us.</p> -->
        </div>
      </div>
    </div>
   
    <div class="carousel-spair-padding">
      <div class="hero-body carousel-body-vert-padding">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">

                  
          <div class="item item-toby">
            <img style='height: auto; width: 80%; object-fit: contain' src="static/images/caption.jpg" alt="edit_propagation">
          </div>

          <div class="item item-toby">
            <img style='height: auto; width: 80%; object-fit: contain' src="static/images/vqa.jpg" alt="edit_propagation">
          </div>
        
        </div>
      </div>
    </div>
  </div>
  </div>
</section>


<br>
<br>





<br>



<section class="section grey">
  <div class="columns is-centered">
    <div class="column is-four-fifths">
      <h3 class="title is-3">Acknowledgements</h3>
      <div class="content has-text-justified">
        <p>
          This work was partly supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), 
          and HPC resources of IDRIS under the allocation 2022-[AD011013415] and 2023-[AD011013415R1] made by GENCI. 
          The authors would like to thank Theophane Vallaeys for fruitful discussion.
        </p>
      </div>
    </div>
  </div>

</section>

 <br>


<section class="section" id="BibTeX">
  <div class="columns is-centered">
    <div class="column is-four-fifths">
      <h3 class="title is-3">BibTeX</h3>
    </div>
 </div>
  <div class="container is-max-desktop content">
    <pre><code>
      @article{shukor2023ep,
        title={eP-ALM: Efficient Perceptual Augmentation of Language Models},
        author={Shukor, Mustafa and Dancette, Corentin and Cord, Matthieu},
        journal={arXiv preprint arXiv:2303.11403},
        year={2023}
      }      
</code></pre>
  </div>
</section>


<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://diffusionfeatures.github.io/">diffusionfeatures</a>.
        </div>
      </div>
    </div>
</footer>

</body>
</html>
